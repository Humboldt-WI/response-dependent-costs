{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion data Bayesian Causal Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xbcausalforest import XBCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is the result of a simulation on top of some real data. The treatment effect on the purchase amount is nonlinear dependent on some X variables and lies roughly between -40 and 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested to estimate the treatment effect on the purchase/response `tau_response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"../data/fashionB_clean_nonlinear.csv\")\n",
    "#X = data.copy()\n",
    "\n",
    "# PARAMETERS\n",
    "SEED=42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "c = X.pop('converted')\n",
    "z = X.pop('TREATMENT')\n",
    "y = X.pop('checkoutAmount')\n",
    "tau_conversion = X.pop('TREATMENT_EFFECT_CONVERSION')\n",
    "tau_basket = X.pop('TREATMENT_EFFECT_BASKET')\n",
    "tau_response = X.pop('TREATMENT_EFFECT_RESPONSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns that are not binary with max=1\n",
    "num_columns = np.where(X.columns[(X.max(axis=0) != 1)])[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = StratifiedKFold(n_splits=2, shuffle=True, random_state=SEED)\n",
    "cz_groups = 2*z+c # Groups 0-4 depending on combinations [0,1]x[0,1]\n",
    "folds = list(splitter.split(X, cz_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and validation data\n",
    "split = folds[0]\n",
    "X_val, y_val, c_val, z_val, tau_conversion_val, tau_basket_val, tau_response_val = [obj.to_numpy().astype(float)[split[1]] for obj in [X, y, c, z, tau_conversion, tau_basket, tau_response]]\n",
    "X, y, c, z, tau_conversion, tau_basket, tau_response = [obj.to_numpy().astype(float)[split[0]] for obj in [X, y, c, z, tau_conversion, tau_basket, tau_response]]\n",
    "\n",
    "\n",
    "# Normalize the data\n",
    "ct = ColumnTransformer([\n",
    "    # (name, transformer, columns)\n",
    "    # Transformer for categorical variables\n",
    "    #(\"onehot\",\n",
    "    #     OneHotEncoder(categories='auto', handle_unknown='ignore', ),\n",
    "    #     cat_columns),\n",
    "    # Transformer for numeric variables\n",
    "    (\"num_preproc\", StandardScaler(), num_columns)\n",
    "    ],\n",
    "    # what to do with variables not specified in the indices?\n",
    "    remainder=\"passthrough\")\n",
    "\n",
    "X = ct.fit_transform(X)\n",
    "X_val = ct.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treatment indicator 1/0 yes/no\n",
    "z= z.astype('int32')\n",
    "\n",
    "# scale response variable\n",
    "meany = np.mean(y)\n",
    "sdy = np.std(y)\n",
    "y_norm = y - np.mean(y)\n",
    "y_norm = y_norm / sdy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters include those set for the Bayesian Tree Ensemble and those specific to the specification to estimate the heterogeneous treatment effect. \n",
    "\n",
    "The first important insight is that the specifcation of the causal model is a sum of two parts. The first part is a BART to estimate the expected outcome from a set of covariates and, if required, an estimate of the probability to receive treatment. The parameters for this BART model are denoted with `pr` for 'prognostic', e.g. `num_trees_pr`.    \n",
    "The second part is a BART model that estimates the treatment effect conditional on the same or a different set of covariates, with its parameters denoted by `trt` as in 'treatment'.\n",
    "\n",
    "The ensemble for the treatment effect may be a bit smaller/more restricted. \n",
    "\n",
    "Parameters that control the BART estimation:\n",
    "- `num_cutpoints`: Number of cutpoints that are tested as splits for continuous covariates\n",
    "- `num_sweeps` (and `burnin`): \n",
    "\n",
    "Parameters that control the size of the BART ensemble:\n",
    "- `num_trees`: Number of trees in the ensemble. Probably somewhere between 50 and 250. \n",
    "- `Nmin`: Minimum number of samples in the final leaves\n",
    "- `alpha`, `beta`: Control the tree depth by setting a prior for the probability that the current leaf is a final leaf formalized by $p(\\text{leaf at depth t is not final leaf}) = \\alpha(1+d)^{-\\beta}$. A lower `alpha` and higher `beta` make the trees more shallow. Chipman et al. (2010) suggest $\\alpha=0.95, \\beta=2$. `alpha` is probably reasonable between [0.5,0.95] and `beta` between [1, 2].\n",
    "- `tau`: Prior on the variance in the leaf for each tree. Hahn et al. propose to scale the prior with some factor the variance of the outcome divided by the number of trees. The factor might be somewhere between 0.1 and 10 but I have very little intuition there. \n",
    "- `mtry`: Number of variables to try at each split. Probably between half and all of them, closer to all of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe the important parameters are `alpha`, `beta`,`tau`. In particular for `tau` I don't have a good intuition. \n",
    "Maybe important are `num_trees`, `num_sweeps`, `mtry`. \n",
    "\n",
    "**Please help me by tuning the parameters until we find a reasonable good estimate where the actual treatment effect distribution and the predicted treatment effects are close**. in particular, i've seen many results where the predicted effects don't reach fully down into the negative or positive effects. For example, the many negative effects are between -1 and -10 but the smallest predicted negative effect would be -1. \n",
    "\n",
    "Tuning using grid search would be nice but structured manual tuning might be sufficient. in the latter case, please log the results somewhere so we can compare them afterwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def make_grid(iterables):\n",
    "    \"\"\"\n",
    "    Create a list of tuples from the combinations of elements in several lists of different length\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    list of tuples or list of dicts\n",
    "      if iterables is a dictionary of lists, the output is a list of dictionaries with the same keys and \n",
    "      values of each combination, e.g. {\"A\":[1,2], \"B\":[3]} -> [{\"A\":1, \"B\":3}, {\"A\":2, \"B\":3}]\n",
    "    \"\"\"\n",
    "    if isinstance(iterables,dict):\n",
    "        out = list(product(*iterables.values()))\n",
    "        out = [dict(zip(iterables.keys(), x)) for x in out]\n",
    "    else:\n",
    "        out = list(product(*iterables))\n",
    " \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_categorical_vars = int(X.shape[1] - len(num_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'tau_pr': [0.1, 1, 10, 100],\n",
    "    'tau_trt':[0.1, 1, 10, 100],\n",
    "    'alpha': [0.95],\n",
    "    'beta': [1.5, 2],\n",
    "    'num_trees': [50, 100, 200],\n",
    "    'num_sweeps': [100],\n",
    "}\n",
    "\n",
    "param_grid = make_grid(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, z_val = z.astype(np.int32), z_val.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_pr = X.shape[1] # Number of vars to consider in each split\n",
    "d_trt = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "BURNIN = 20 # Drop some samples in a warmup period of the sampler\n",
    "\n",
    "results = []\n",
    "pred = []\n",
    "for i, param_set in enumerate(param_grid):\n",
    "    cf = XBCF(\n",
    "        #model=\"Normal\",\n",
    "        parallel=True, \n",
    "        #p_categorical_pr= n_categorical_vars,\n",
    "        #p_categorical_trt= n_categorical_vars,\n",
    "          num_sweeps=param_set['num_sweeps'], \n",
    "          burnin=BURNIN,\n",
    "          max_depth=250,\n",
    "            num_trees_pr=param_set['num_trees'],\n",
    "            num_trees_trt=param_set['num_trees'],\n",
    "            num_cutpoints=100,\n",
    "            mtry_pr=d_pr,\n",
    "            mtry_trt=d_trt,\n",
    "            Nmin=1,\n",
    "            tau_pr =  param_set['tau_pr']*np.var(y)/param_set['num_trees'],\n",
    "            tau_trt = param_set['tau_trt']*np.var(y)/param_set['num_trees'], \n",
    "            no_split_penality=\"auto\",\n",
    "            alpha_pr= 0.95, # shrinkage (splitting probability)\n",
    "            beta_pr= 2, # shrinkage (tree depth)\n",
    "            alpha_trt= param_set['alpha'], # shrinkage for treatment part\n",
    "            beta_trt= param_set['beta'],\n",
    "             )\n",
    "    \n",
    "    cf = cf.fit(\n",
    "    x_t=X, # Covariates treatment effect\n",
    "    x=X, # Covariates outcome (including propensity score)\n",
    "    y=y_norm,  # Outcome\n",
    "    z=z, # Treatment group\n",
    "    #p_cat=n_categorical_vars\n",
    "    )\n",
    "    \n",
    "    tauhats_test = cf.predict(X)\n",
    "\n",
    "    # Due to the scaling of y, the y and tau predictions need to be unscaled\n",
    "\n",
    "    b = cf.b.transpose()\n",
    "    a = cf.a.transpose()\n",
    "\n",
    "    thats = sdy * cf.tauhats * (b[1] - b[0])\n",
    "    thats_mean = np.mean(thats[:, BURNIN:], axis=1)\n",
    "    yhats = cf.muhats * a + cf.tauhats * (b[1] - b[0])\n",
    "    yhats_mean = np.mean(yhats[:, BURNIN:], axis=1)\n",
    "    \n",
    "    param_set['error'] = np.mean(abs(tau_response - thats_mean))\n",
    "    \n",
    "    results.append(param_set)\n",
    "    pred.append(thats_mean)\n",
    "    \n",
    "    if i%10==0:\n",
    "        print(\"Done iteration\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "results.to_csv(\"../results/xbcf_tuning_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows=999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.iloc[results.error.idxmin(),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(y_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myXBCF(XBCF):\n",
    "    def fit(self, x_t, x, y, z, p_cat=0):\n",
    "        z= z.astype('int32')\n",
    "\n",
    "        self.sdy = np.std(y)\n",
    "        y = y - np.mean(y)\n",
    "        y = y / self.sdy\n",
    "        self.y = y\n",
    "\n",
    "        super().fit(x_t, x, y, z, p_cat=p_cat)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, *args,**kwargs):\n",
    "        tauhats = super().predict(*args,**kwargs)\n",
    "        \n",
    "        b = self.b.transpose()\n",
    "        a = self.a.transpose()\n",
    "\n",
    "        thats = self.sdy * tauhats * (b[1] - b[0])\n",
    "        thats_mean = np.mean(thats[:, self.get_params()['burnin']:], axis=1)\n",
    "        \n",
    "        return thats_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cf = myXBCF(random_seed=123, set_random_seed=True,\n",
    "    #model=\"Normal\",\n",
    "    parallel=True, \n",
    "    #p_categorical_pr= n_categorical_vars,\n",
    "    #p_categorical_trt= n_categorical_vars,\n",
    "      num_sweeps=40, \n",
    "      burnin=20,\n",
    "      max_depth=250,\n",
    "        num_trees_pr=100,\n",
    "        num_trees_trt=50,\n",
    "        num_cutpoints=100,\n",
    "        mtry_pr=d_pr,\n",
    "        mtry_trt=d_trt,\n",
    "        Nmin=50,\n",
    "        tau_pr =  0.1/50,\n",
    "        tau_trt = 0.1/50, \n",
    "        no_split_penality=\"auto\",\n",
    "        alpha_pr= 0.95, # shrinkage (splitting probability)\n",
    "        beta_pr= 2, # shrinkage (tree depth)\n",
    "        alpha_trt= 0.95, # shrinkage for treatment part\n",
    "        beta_trt= 2,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in method 'XBCFcpp__fit', argument 12 of type 'size_t'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-92049491161c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_t, x, y, z, p_cat)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_cat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/uplift/lib/python3.8/site-packages/xbcausalforest/xbcf_python.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_t, x, y, z, p_cat)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;31m# print(type(fit_z[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# fit #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xbcf_cpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_x_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mmuhats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xbcf_cpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_muhats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_sweeps\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfit_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/uplift/lib/python3.8/site-packages/xbcausalforest/xbcf_cpp_.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, n_t, n_p, n_y, n_z, p_cat)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_t\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"int\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"int\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"int\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_z\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"int\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_cat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"size_t\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"void\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_xbcf_cpp_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXBCFcpp__fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_t\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"int\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"void\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in method 'XBCFcpp__fit', argument 12 of type 'size_t'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "my_cf.fit(\n",
    "x_t=X, # Covariates treatment effect\n",
    "x=X, # Covariates outcome (including propensity score)\n",
    "y=y,  # Outcome\n",
    "z=z, # Treatment group\n",
    "p_cat=1.*n_categorical_vars\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 45s, sys: 8.08 s, total: 1min 53s\n",
      "Wall time: 29.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XBCF(num_sweeps = 40, burnin = 20, max_depth = 250, Nmin = 50, num_cutpoints = 20, no_split_penality = 2.995732273553991, mtry_pr = 61, mtry_trt = 61, p_categorical_pr = 0, p_categorical_trt = 0, num_trees_pr = 10, alpha_pr = 0.95, beta_pr = 2.0, tau_pr = 4.899426354183893, kap_pr = 16.0, s_pr = 4.0, pr_scale = False, num_trees_trt = 10, alpha_trt = 0.95, beta_trt = 2.0, tau_trt = 4.899426354183893, kap_trt = 16.0, s_trt = 4.0, trt_scale = False, verbose = False, parallel = True, set_random_seed = True, random_seed = 123, sample_weights_flag = True, a_scaling = True, b_scaling = True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cf = XBCF(random_seed=123,set_random_seed=True, \n",
    "    #model=\"Normal\",\n",
    "    parallel=True, \n",
    "    #p_categorical_pr= n_categorical_vars,\n",
    "    #p_categorical_trt= n_categorical_vars,\n",
    "      num_sweeps=40, \n",
    "      burnin=20,\n",
    "      max_depth=250,\n",
    "        num_trees_pr=10,\n",
    "        num_trees_trt=10,\n",
    "        num_cutpoints=20,\n",
    "        mtry_pr=d_pr,\n",
    "        mtry_trt=d_trt,\n",
    "        Nmin=50,\n",
    "        tau_pr =  0.1*np.var(y)/20,\n",
    "        tau_trt = 0.1*np.var(y)/20, \n",
    "        no_split_penality=\"auto\",\n",
    "        alpha_pr= 0.95, # shrinkage (splitting probability)\n",
    "        beta_pr= 2, # shrinkage (tree depth)\n",
    "        alpha_trt= 0.95, # shrinkage for treatment part\n",
    "        beta_trt= 2,\n",
    "         )\n",
    "\n",
    "cf.fit(\n",
    "x_t=X, # Covariates treatment effect\n",
    "x=X, # Covariates outcome (including propensity score)\n",
    "y=y_norm,  # Outcome\n",
    "z=z, # Treatment group\n",
    "#p_cat=n_categorical_vars\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tauhats = cf.predict(X_val)\n",
    "\n",
    "# Due to the scaling of y, the y and tau predictions need to be unscaled\n",
    "\n",
    "b = cf.b.transpose()\n",
    "a = cf.a.transpose()\n",
    "\n",
    "thats = sdy * tauhats * (b[1] - b[0])\n",
    "thats_mean = np.mean(thats[:, 5:], axis=1)\n",
    "yhats = cf.muhats * a + cf.tauhats * (b[1] - b[0])\n",
    "yhats_mean = np.mean(yhats[:, 5:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_thats_mean = my_cf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.98262399],\n",
       "       [0.98262399, 1.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef([thats_mean, my_thats_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7589389223443418"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cf.a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7589389223443418"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.30311918701998"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.30311918701998"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_cf.sdy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some parameters the sampling fails and the internal results are NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the difference between `tau_response` an `thats_mean` to be as small as possible for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(abs(tau_response - thats_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_response.mean(),  thats_mean.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tau_response, thats_mean);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_response.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(tau_response)\n",
    "sns.kdeplot(thats_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhats = cf.muhats * a + cf.tauhats * (b[1] - b[0])\n",
    "yhats_mean = np.mean(yhats[:, (BURNIN) :], axis=1)\n",
    "plt.scatter(y[c==1], yhats_mean[c==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.mean(), yhats.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:uplift]",
   "language": "python",
   "name": "conda-env-uplift-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
